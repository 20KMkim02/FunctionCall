{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = client.complete(\n",
    "    messages=[\n",
    "        UserMessage(content=\"What is the capital of France?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function call simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import (\n",
    "    AssistantMessage,\n",
    "    ChatCompletionsToolCall,\n",
    "    ChatCompletionsToolDefinition,\n",
    "    CompletionsFinishReason,\n",
    "    FunctionDefinition,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    "    UserMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions=[\n",
    "#     # Jason file that contian ; name , description paramter\n",
    "#     #                                              |---type , properties ,required\n",
    "#     #                                                         |---location ,unit\n",
    "#     {\n",
    "#         \"name\": \"get_current_weather\",\n",
    "#         \"description\": \"Get the current weather in a given location\",\n",
    "#         \"parameters\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"properties\": {\n",
    "#                 \"location\": {\n",
    "#                     \"type\": \"string\",\n",
    "#                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "#                 },\n",
    "#                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "#             },\n",
    "#             \"required\": [\"location\"],\n",
    "#         },\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_temp(location, unit=\"celsius\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"32\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    UserMessage(content=\"I want to know over all temperature of the Thailand\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_temp = ChatCompletionsToolDefinition(\n",
    "    function=FunctionDefinition(\n",
    "        name=\"get_overall_temp\",\n",
    "        description=\"\"\"Return Overall temperature of that location \"\"\",\n",
    "        parameters={\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"Unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "response = client.complete(\n",
    "    messages=messages,\n",
    "    model=model_name,\n",
    "    # temperature=1.0,\n",
    "    # top_p=1.0,\n",
    "    # max_tokens=1000,\n",
    "    tools=[get_temp],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"content_filter_results\": {},\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"location\\\":\\\"Thailand\\\"}\",\n",
      "              \"name\": \"get_overall_temp\"\n",
      "            },\n",
      "            \"id\": \"call_knm8JfW88K6fEYwuyrlKIXoZ\",\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1737700935,\n",
      "  \"id\": \"chatcmpl-At7gNtufUwOy1iOG6IP5OgBxGIKEH\",\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"system_fingerprint\": \"fp_5154047bf2\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 16,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens\": 81,\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 97\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": {\"location\": \"Thailand\"}, \"temperature\": \"32\", \"unit\": \"celsius\", \"forecast\": [\"sunny\", \"windy\"]}'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "observation=get_overall_temp(args)\n",
    "get_overall_temp(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": \"get_temp\",\n",
    "            \"content\": observation,\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall temperature in Thailand is around 32Â°C. The weather is expected to be sunny and windy. If you need more specific information or forecasts for different regions within Thailand, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = client.complete(\n",
    "    messages=messages,\n",
    "    model=model_name,\n",
    "    # temperature=1.0,\n",
    "    # top_p=1.0,\n",
    "    # max_tokens=1000,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI #à¸à¹à¸­à¸à¹à¸à¹à¸AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/kgw71nq93vd05bh5_fkk3_xr0000gn/T/ipykernel_40574/2651002058.py:4: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm = AzureChatOpenAI(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/chat_models/azure_openai.py:174: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://models.inference.ai.azure.com to https://models.inference.ai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "api_key = os.environ['GITHUB_TOKEN']\n",
    "api_endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=api_endpoint,\n",
    "    # deployment_name=\"your_deployment_name\",  # Adjust based on your Azure setup\n",
    "    model=\"gpt-4o-mini\",  # Specify your model name\n",
    "    openai_api_version=\"2023-03-15-preview\"  # Ensure it matches your Azure API version\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me various short jokes about {topic}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# à¸ªà¸£à¹à¸²à¸Chain à¸­à¸­à¸à¸¡à¸²\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are some short jokes about Natural Language Processing (NLP):\n",
      "\n",
      "1. Why did the NLP model break up with its partner?\n",
      "   Because they just didnât understand each other!\n",
      "\n",
      "2. What did the NLP model say to the unstructured data?\n",
      "   \"You complete me!\"\n",
      "\n",
      "3. Why did the computer go to therapy?\n",
      "   It had too many unresolved dependencies!\n",
      "\n",
      "4. How do NLP researchers stay organized?\n",
      "   They always keep their sentences in order!\n",
      "\n",
      "5. Why did the NLP model get kicked out of the party?\n",
      "   It kept trying to tokenize everyone!\n",
      "\n",
      "6. What do you call a sad NLP model?\n",
      "   A token of sadness!\n",
      "\n",
      "7. Why did the word go to school?\n",
      "   To improve its vocab-ular-ity!\n",
      "\n",
      "8. How does an NLP model apologize?\n",
      "   \"Iâm sorry; that was a parsing error!\"\n",
      "\n",
      "9. Why don't NLP models tell secrets?\n",
      "   Because they can't keep their embeddings to themselves!\n",
      "\n",
      "10. What did one vector say to the other?\n",
      "    \"You complete my cosine similarity!\"\n",
      "\n",
      "Hope these brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "# Run the chain with a topic\n",
    "response = chain.run({\"topic\": \"NLP\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### à¸ªà¸²à¸¡à¸²à¸£à¸à¹à¸­à¸² Chian à¸à¸µà¹ à¹à¸à¹à¸à¹à¸à¸±à¸à¸£à¸°à¸à¸ RAG à¸à¹à¸­à¸à¸à¸µà¹à¸à¸°Chainà¹à¸à¹à¸²à¹à¸à¹à¸ LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
