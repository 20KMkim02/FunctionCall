{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = client.complete(\n",
    "    messages=[\n",
    "        UserMessage(content=\"What is the capital of France?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function call simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import (\n",
    "    AssistantMessage,\n",
    "    ChatCompletionsToolCall,\n",
    "    ChatCompletionsToolDefinition,\n",
    "    CompletionsFinishReason,\n",
    "    FunctionDefinition,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    "    UserMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions=[\n",
    "#     # Jason file that contian ; name , description paramter\n",
    "#     #                                              |---type , properties ,required\n",
    "#     #                                                         |---location ,unit\n",
    "#     {\n",
    "#         \"name\": \"get_current_weather\",\n",
    "#         \"description\": \"Get the current weather in a given location\",\n",
    "#         \"parameters\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"properties\": {\n",
    "#                 \"location\": {\n",
    "#                     \"type\": \"string\",\n",
    "#                     \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "#                 },\n",
    "#                 \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "#             },\n",
    "#             \"required\": [\"location\"],\n",
    "#         },\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_temp(location, unit=\"celsius\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"32\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    UserMessage(content=\"I want to know over all temperature of the Thailand\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_temp = ChatCompletionsToolDefinition(\n",
    "    function=FunctionDefinition(\n",
    "        name=\"get_overall_temp\",\n",
    "        description=\"\"\"Return Overall temperature of that location \"\"\",\n",
    "        parameters={\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"Unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "response = client.complete(\n",
    "    messages=messages,\n",
    "    model=model_name,\n",
    "    # temperature=1.0,\n",
    "    # top_p=1.0,\n",
    "    # max_tokens=1000,\n",
    "    tools=[get_temp],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"content_filter_results\": {},\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"location\\\":\\\"Thailand\\\"}\",\n",
      "              \"name\": \"get_overall_temp\"\n",
      "            },\n",
      "            \"id\": \"call_knm8JfW88K6fEYwuyrlKIXoZ\",\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1737700935,\n",
      "  \"id\": \"chatcmpl-At7gNtufUwOy1iOG6IP5OgBxGIKEH\",\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"system_fingerprint\": \"fp_5154047bf2\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 16,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens\": 81,\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 97\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": {\"location\": \"Thailand\"}, \"temperature\": \"32\", \"unit\": \"celsius\", \"forecast\": [\"sunny\", \"windy\"]}'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "observation=get_overall_temp(args)\n",
    "get_overall_temp(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": \"get_temp\",\n",
    "            \"content\": observation,\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall temperature in Thailand is around 32°C. The weather is expected to be sunny and windy. If you need more specific information or forecasts for different regions within Thailand, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = client.complete(\n",
    "    messages=messages,\n",
    "    model=model_name,\n",
    "    # temperature=1.0,\n",
    "    # top_p=1.0,\n",
    "    # max_tokens=1000,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI #ต้องเป็นAzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/kgw71nq93vd05bh5_fkk3_xr0000gn/T/ipykernel_40574/2651002058.py:4: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm = AzureChatOpenAI(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/chat_models/azure_openai.py:174: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://models.inference.ai.azure.com to https://models.inference.ai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "api_key = os.environ['GITHUB_TOKEN']\n",
    "api_endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=api_endpoint,\n",
    "    # deployment_name=\"your_deployment_name\",  # Adjust based on your Azure setup\n",
    "    model=\"gpt-4o-mini\",  # Specify your model name\n",
    "    openai_api_version=\"2023-03-15-preview\"  # Ensure it matches your Azure API version\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me various short jokes about {topic}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# สร้างChain ออกมา\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are some short jokes about Natural Language Processing (NLP):\n",
      "\n",
      "1. Why did the NLP model break up with its partner?\n",
      "   Because they just didn’t understand each other!\n",
      "\n",
      "2. What did the NLP model say to the unstructured data?\n",
      "   \"You complete me!\"\n",
      "\n",
      "3. Why did the computer go to therapy?\n",
      "   It had too many unresolved dependencies!\n",
      "\n",
      "4. How do NLP researchers stay organized?\n",
      "   They always keep their sentences in order!\n",
      "\n",
      "5. Why did the NLP model get kicked out of the party?\n",
      "   It kept trying to tokenize everyone!\n",
      "\n",
      "6. What do you call a sad NLP model?\n",
      "   A token of sadness!\n",
      "\n",
      "7. Why did the word go to school?\n",
      "   To improve its vocab-ular-ity!\n",
      "\n",
      "8. How does an NLP model apologize?\n",
      "   \"I’m sorry; that was a parsing error!\"\n",
      "\n",
      "9. Why don't NLP models tell secrets?\n",
      "   Because they can't keep their embeddings to themselves!\n",
      "\n",
      "10. What did one vector say to the other?\n",
      "    \"You complete my cosine similarity!\"\n",
      "\n",
      "Hope these brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "# Run the chain with a topic\n",
    "response = chain.run({\"topic\": \"NLP\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### สามารถเอา Chian นี้ ไปใช้กับระบบ RAG ก่อนที่จะChainเข้าไปใน LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI Embeddings model\n",
    "azure_embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=api_endpoint,\n",
    "    # deployment=deployment_name,  # Specify your embedding model's deployment\n",
    "    openai_api_type=\"azure\",  # Azure-specific setting\n",
    "    openai_api_version=\"2023-03-15-preview\"  # Azure API version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import docarray python package. Please install it with `pip install docarray`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/vectorstores/docarray/base.py:19\u001b[0m, in \u001b[0;36m_check_docarray_import\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     da_version \u001b[38;5;241m=\u001b[39m docarray\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mharrison worked at kensho\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbears like to eat honey\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDaisy like dancing in the moon light\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlondie love drinking longisland vodka 38\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mDocArrayInMemorySearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mazure_embeddings\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/vectorstores/docarray/in_memory.py:67\u001b[0m, in \u001b[0;36mDocArrayInMemorySearch.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     54\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DocArrayInMemorySearch:\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create an DocArrayInMemorySearch store and insert data.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m        DocArrayInMemorySearch Vector Store\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     store\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m store\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/vectorstores/docarray/in_memory.py:40\u001b[0m, in \u001b[0;36mDocArrayInMemorySearch.from_params\u001b[0;34m(cls, embedding, metric, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_params\u001b[39m(\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     30\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DocArrayInMemorySearch:\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize DocArrayInMemorySearch store.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m        **kwargs: Other keyword arguments to be passed to the get_doc_cls method.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[43m_check_docarray_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryExactNNIndex\n\u001b[1;32m     43\u001b[0m     doc_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_doc_cls(space\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/vectorstores/docarray/base.py:29\u001b[0m, in \u001b[0;36m_check_docarray_import\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use the DocArrayHnswSearch VectorStore the docarray \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion >=0.32.0 is expected, received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocarray\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo upgrade, please run: `pip install -U docarray`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import docarray python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install docarray`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import docarray python package. Please install it with `pip install docarray`."
     ]
    }
   ],
   "source": [
    "texts = [\"harrison worked at kensho\", \"bears like to eat honey\",\n",
    "         'Daisy like dancing in the moon light','Blondie love drinking longisland vodka 38%']\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    texts,\n",
    "    embedding=azure_embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
